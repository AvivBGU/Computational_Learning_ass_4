{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.4.7)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (3.10.8)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (4.13.0.90)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (12.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.17.0)\n",
      "Requirement already satisfied: torch<2.10,>=1.8.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from ultralytics) (7.2.1)\n",
      "Requirement already satisfied: polars>=0.20.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (1.37.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: polars-runtime-32==1.37.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from polars>=0.20.0->ultralytics) (1.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (2026.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<2.10,>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.4.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.0.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\aviv metz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aviv metz\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.datasets import Flowers102\n",
    "from torch.utils.data import Subset, ConcatDataset\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Optional, Callable\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what device is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT: str = \"./data\" \n",
    "TRAIN_RELATIVE_SIZE: float = 0.5\n",
    "VALIDATION_RELATIVE_SIZE: float = 0.25\n",
    "TEST_RELATIVE_SIZE: float = 0.25\n",
    "YOLOV5_MODEL: str = 'yolov5s'\n",
    "DATA_SPLIT_SEED_LIST: list[int] = [42, 43, 44]\n",
    "EXPECTED_IMAGE_SIZE: tuple[int, int] = (224, 224)\n",
    "IMAGENET_STD: list[float] = [0.229, 0.224, 0.225]\n",
    "IMAGENET_MEAN: list[float] = [0.485, 0.456, 0.406]\n",
    "PATIENCE_EPOCHS: int = 10\n",
    "IMPROVEMENT_DELTA: float = 0.01\n",
    "MAX_EPOCHS: int = 100\n",
    "BATCH_SIZE: int = 32\n",
    "NUM_WORKERS_FOR_DATALOADER: int = 0  # Set to 0 for Windows compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset to be processed later.\n",
    "full_dataset = torch.utils.data.ConcatDataset([\n",
    "    Flowers102(root=DATASET_ROOT, split=\"train\", download=True),\n",
    "    Flowers102(root=DATASET_ROOT, split=\"val\",   download=True),\n",
    "    Flowers102(root=DATASET_ROOT, split=\"test\",  download=True),\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8189\n",
      "Classes: 102\n",
      "Min per class: 40\n",
      "Max per class: 258\n",
      "Imbalance ratio (max/min): 6.45\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "\n",
    "for single_dataset in full_dataset.datasets:  # full_dataset is ConcatDataset\n",
    "    all_labels.extend(single_dataset._labels)  # Flowers102 stores labels here\n",
    "counts = Counter(all_labels)\n",
    "\n",
    "num_classes = len(counts)\n",
    "total = len(all_labels)\n",
    "min_c = min(counts.values())\n",
    "max_c = max(counts.values())\n",
    "\n",
    "print(f\"Total samples: {total}\")\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Min per class: {min_c}\")\n",
    "print(f\"Max per class: {max_c}\")\n",
    "print(f\"Imbalance ratio (max/min): {max_c/min_c:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added utility functions to properly separate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_labels_from_concat_dataset(concat_dataset: ConcatDataset):\n",
    "    \"\"\"Collect labels from each underlying dataset without loading images.\"\"\"\n",
    "    all_labels_list = []\n",
    "\n",
    "    for single_dataset in concat_dataset.datasets:\n",
    "        if hasattr(single_dataset, \"_labels\"):          # Flowers102\n",
    "            labels_array = np.asarray(single_dataset._labels, dtype=int)\n",
    "        elif hasattr(single_dataset, \"targets\"):        # ImageFolder, CIFAR, etc.\n",
    "            labels_array = np.asarray(single_dataset.targets, dtype=int)\n",
    "        else:  # fallback (slow)\n",
    "            labels_array = np.array(\n",
    "                [single_dataset[i][1] for i in range(len(single_dataset))],\n",
    "                dtype=int\n",
    "            )\n",
    "\n",
    "        all_labels_list.append(labels_array)\n",
    "\n",
    "    return np.concatenate(all_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_concat_dataset(\n",
    "    concat_dataset: ConcatDataset,\n",
    "    split_fractions=(TRAIN_RELATIVE_SIZE, VALIDATION_RELATIVE_SIZE, TEST_RELATIVE_SIZE),\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"Split ConcatDataset into stratified Subsets with same class proportions.\"\"\"\n",
    "\n",
    "    normalized_fractions = np.array(split_fractions, dtype=float)\n",
    "    normalized_fractions = normalized_fractions / normalized_fractions.sum()\n",
    "\n",
    "    all_labels = extract_all_labels_from_concat_dataset(concat_dataset)\n",
    "    random_generator = np.random.default_rng(random_seed)\n",
    "\n",
    "    # group global indices by class\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for global_index, class_label in enumerate(all_labels):\n",
    "        class_to_indices[int(class_label)].append(global_index)\n",
    "\n",
    "    split_indices_per_subset = [[] for _ in range(len(normalized_fractions))]\n",
    "\n",
    "    for class_indices in class_to_indices.values():\n",
    "        class_indices = np.array(class_indices, dtype=int)\n",
    "        random_generator.shuffle(class_indices)\n",
    "\n",
    "        total_class_samples = len(class_indices)\n",
    "        samples_per_split = np.floor(normalized_fractions * total_class_samples).astype(int)\n",
    "\n",
    "        # distribute leftover samples\n",
    "        remainder = total_class_samples - samples_per_split.sum()\n",
    "        for split_id in random_generator.permutation(len(samples_per_split))[:remainder]:\n",
    "            samples_per_split[split_id] += 1\n",
    "\n",
    "        start_pointer = 0\n",
    "        for split_id, count in enumerate(samples_per_split):\n",
    "            split_indices_per_subset[split_id].extend(\n",
    "                class_indices[start_pointer:start_pointer + count].tolist()\n",
    "            )\n",
    "            start_pointer += count\n",
    "\n",
    "    # shuffle each split's indices\n",
    "    for split_list in split_indices_per_subset:\n",
    "        random_generator.shuffle(split_list)\n",
    "\n",
    "    train_subset, val_subset, test_subset = [\n",
    "        Subset(concat_dataset, indices) for indices in split_indices_per_subset\n",
    "    ]\n",
    "\n",
    "    return train_subset, val_subset, test_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added transformations for both image rescale and data augemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(EXPECTED_IMAGE_SIZE, scale=(0.8, 1.0)),  # random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # Random horizontal flip\n",
    "    transforms.RandomAffine( # small random rotations, translations, scaling\n",
    "        degrees=10, \n",
    "        translate=(0.03, 0.03),  \n",
    "        scale=(0.97, 1.03)       \n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN,\n",
    "                         IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "yolo_val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(EXPECTED_IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN,\n",
    "                         IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database to properly handle transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowersDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap your existing dataset that returns:\n",
    "      - image: PIL.Image\n",
    "      - label: int (0..num_classes-1)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset: Dataset, transform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_pil, label = self.base_dataset[idx]\n",
    "        image_pil = self.transform(image_pil)\n",
    "        return image_pil, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataloaders    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    num_workers: int = NUM_WORKERS_FOR_DATALOADER\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 sets of splits, as required in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_splits(\n",
    "    full_dataset: ConcatDataset,\n",
    "    random_seed: int\n",
    ") -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    train_subset, val_subset, test_subset = stratified_split_concat_dataset(\n",
    "        concat_dataset=full_dataset,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    train_dataset = FlowersDataset(train_subset, transform=yolo_train_transform)\n",
    "    val_dataset = FlowersDataset(val_subset, transform=yolo_val_test_transform)\n",
    "    test_dataset = FlowersDataset(test_subset, transform=yolo_val_test_transform)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders_sets_per_seed = []\n",
    "for seed in DATA_SPLIT_SEED_LIST:\n",
    "    train_dataset, val_dataset, test_dataset = create_dataset_splits(full_dataset, random_seed=seed)\n",
    "    train_dataloader, val_dataloader, test_dataloader = create_dataloaders(train_dataset, val_dataset, test_dataset)\n",
    "    loaders_sets_per_seed.append((train_dataloader, val_dataloader, test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience_epochs: int = PATIENCE_EPOCHS, min_delta: float = IMPROVEMENT_DELTA):\n",
    "        self.patience_epochs = patience_epochs\n",
    "        self.min_delta = min_delta\n",
    "        self.best_value: Optional[float] = None\n",
    "        self.epochs_without_improvement: int = 0\n",
    "        self.best_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
    "\n",
    "    def step(self, current_value: float, model: nn.Module) -> tuple[bool, bool]:\n",
    "        \"\"\"\n",
    "        Returns should_stop, improved\n",
    "        \"\"\"\n",
    "        if self.best_value is None: # Only relevant in the first call\n",
    "            self.best_value = current_value\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            return False, True\n",
    "\n",
    "\n",
    "        improved = current_value < (self.best_value - self.min_delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best_value = current_value\n",
    "            self.epochs_without_improvement = 0\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            return False, True\n",
    "\n",
    "        self.epochs_without_improvement += 1\n",
    "        should_stop = self.epochs_without_improvement >= self.patience_epochs\n",
    "        return should_stop, improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running single training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch_train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "\n",
    "    total_loss_value = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_images, batch_targets in train_dataloader:\n",
    "        batch_images = batch_images.to(device, non_blocking=True)\n",
    "        batch_targets = batch_targets.to(device, non_blocking=True).long()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_logits = model(batch_images)\n",
    "        # If your model returns a tuple/dict, adapt here:\n",
    "        if isinstance(batch_logits, (tuple, list)):\n",
    "            batch_logits = batch_logits[0]\n",
    "\n",
    "        batch_loss = loss_function(batch_logits, batch_targets)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = batch_targets.size(0)\n",
    "        total_loss_value += batch_loss.item() * batch_size\n",
    "\n",
    "        batch_predicted = batch_logits.argmax(dim=1)\n",
    "        total_correct += (batch_predicted == batch_targets).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    average_loss = total_loss_value / max(1, total_samples)\n",
    "    average_accuracy = total_correct / max(1, total_samples)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running single validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_one_epoch_validation(\n",
    "    model: nn.Module,\n",
    "    val_dataloader: DataLoader,\n",
    "    loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_value = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_images, batch_targets in val_dataloader:\n",
    "        batch_images = batch_images.to(device, non_blocking=True)\n",
    "        batch_targets = batch_targets.to(device, non_blocking=True).long()\n",
    "\n",
    "        batch_logits = model(batch_images)\n",
    "        if isinstance(batch_logits, (tuple, list)):\n",
    "            batch_logits = batch_logits[0]\n",
    "\n",
    "        batch_loss = loss_function(batch_logits, batch_targets)\n",
    "\n",
    "        batch_size = batch_targets.size(0)\n",
    "        total_loss_value += batch_loss.item() * batch_size\n",
    "\n",
    "        batch_predicted = batch_logits.argmax(dim=1)\n",
    "        total_correct += (batch_predicted == batch_targets).sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    average_loss = total_loss_value / max(1, total_samples)\n",
    "    average_accuracy = total_correct / max(1, total_samples)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with early stopping that returns the model that acheived the best results instead of the latest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    optimizer_factory: Callable[[nn.Module], torch.optim.Optimizer],\n",
    "    device: torch.device,\n",
    "    max_epochs: int = MAX_EPOCHS,\n",
    "    improvement_delta: float = IMPROVEMENT_DELTA,\n",
    "    patience_epochs: int = PATIENCE_EPOCHS\n",
    ") -> nn.Module:\n",
    "    epoch_loss: list[float] = list()\n",
    "    epoch_acc: list[float] = list()\n",
    "    epoch_val_loss: list[float] = list()\n",
    "    epoch_val_acc: list[float] = list()\n",
    "    optimizer = optimizer_factory(model)\n",
    "    early_stopper = EarlyStopper(patience_epochs=patience_epochs, min_delta=improvement_delta)\n",
    "    for epoch_index in range(1, max_epochs + 1):\n",
    "        train_loss, train_accuracy = run_one_epoch_train(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "        val_loss, val_accuracy = run_one_epoch_validation(\n",
    "            model=model,\n",
    "            val_dataloader=val_dataloader,\n",
    "            loss_function=loss_function,\n",
    "            device=device,\n",
    "        )\n",
    "        epoch_loss.append(train_loss)\n",
    "        epoch_acc.append(train_accuracy)\n",
    "        epoch_val_loss.append(val_loss)\n",
    "        epoch_val_acc.append(val_accuracy)\n",
    "        print(\n",
    "            f\"Epoch {epoch_index:03d} | \"\n",
    "            f\"train_loss={train_loss:.6f}, train_acc={train_accuracy:.4f} | \"\n",
    "            f\"val_loss={val_loss:.6f}, val_acc={val_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        should_stop, improved = early_stopper.step(current_value=val_loss, model=model)\n",
    "        if not improved:\n",
    "            print(\n",
    "                f\"No improvement in val_loss for {early_stopper.epochs_without_improvement} \"\n",
    "                f\"out of {patience_epochs} allowed epochs.\"\n",
    "            )\n",
    "        if should_stop:\n",
    "            print(\n",
    "                f\"Early stopping: no val_loss improvement >= {improvement_delta} \"\n",
    "                f\"for {patience_epochs} epochs.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    if early_stopper.best_state_dict is not None:\n",
    "        model.load_state_dict(early_stopper.best_state_dict)\n",
    "        print(\"Loaded best model weights (by validation loss).\")\n",
    "\n",
    "    return model, epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_function: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True).long()\n",
    "\n",
    "            logits = model(images)\n",
    "            if isinstance(logits, (tuple, list)):  # in case model returns extra outputs\n",
    "                logits = logits[0]\n",
    "\n",
    "            loss = loss_function(logits, targets)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == targets).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Yolo, classification variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: gitpython>=3.1.30 in d:\\anaconda\\lib\\site-packages (3.1.43)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in d:\\anaconda\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\anaconda\\lib\\site-packages (from gitpython>=3.1.30) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in d:\\anaconda\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (4.0.0)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  2.0s\n",
      "WARNING \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2026-1-26 Python-3.11.0 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 2070 SUPER, 8192MiB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yolo_model = torch.hub.load(\n",
    "    \"ultralytics/yolov5\",\n",
    "    \"custom\",\n",
    "    path=f\"{YOLOV5_MODEL}-cls.pt\",   # classification checkpoint\n",
    "    autoshape=False,        # important for training\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing classifier head with a different classifier due to different categories and nubmer of categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced: model.model.9.linear -> Linear(1280, 102)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetectMultiBackend(\n",
       "  (model): ClassificationModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C3(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv3): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C3(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv3): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C3(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv3): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C3(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv3): Conv(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Classify(\n",
       "        (conv): Conv(\n",
       "          (conv): Conv2d(512, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (drop): Dropout(p=0.0, inplace=True)\n",
       "        (linear): Linear(in_features=1280, out_features=102, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_last_linear_layer(model: nn.Module, num_classes: int) -> nn.Module:\n",
    "    last_linear_name = None\n",
    "    last_linear_module = None\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            last_linear_name = module_name\n",
    "            last_linear_module = module\n",
    "\n",
    "    if last_linear_module is None:\n",
    "        raise RuntimeError(\"No nn.Linear layer found to replace.\")\n",
    "\n",
    "    new_linear_layer = nn.Linear(last_linear_module.in_features, num_classes)\n",
    "\n",
    "    # set by walking to the parent module\n",
    "    parent = model\n",
    "    name_parts = last_linear_name.split(\".\")\n",
    "    for part in name_parts[:-1]:\n",
    "        parent = getattr(parent, part)\n",
    "    setattr(parent, name_parts[-1], new_linear_layer)\n",
    "\n",
    "    print(f\"Replaced: {last_linear_name} -> Linear({last_linear_module.in_features}, {num_classes})\")\n",
    "    return model\n",
    "\n",
    "# usage\n",
    "yolo_model = replace_last_linear_layer(yolo_model, num_classes)\n",
    "yolo_model = yolo_model.to(device)\n",
    "yolo_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data regarding number of parameters in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 4303142\n",
      "Trainable params: 130662\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(parameter.numel() for parameter in yolo_model.parameters())\n",
    "trainable_params = sum(parameter.numel() for parameter in yolo_model.parameters() if parameter.requires_grad)\n",
    "\n",
    "print(\"Total params:\", total_params)\n",
    "print(\"Trainable params:\", trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory is used since it cannot be used before the model is properly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_factory(model):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_optimizer = optimizer_factory\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=3.549493, train_acc=0.3101 | val_loss=2.581550, val_acc=0.5465\n",
      "Epoch 002 | train_loss=2.009889, train_acc=0.7052 | val_loss=1.611691, val_acc=0.7639\n",
      "Epoch 003 | train_loss=1.303480, train_acc=0.8287 | val_loss=1.171780, val_acc=0.8382\n",
      "Epoch 004 | train_loss=0.943822, train_acc=0.8816 | val_loss=0.914688, val_acc=0.8760\n",
      "Epoch 005 | train_loss=0.737549, train_acc=0.9143 | val_loss=0.791420, val_acc=0.8756\n",
      "Epoch 006 | train_loss=0.607203, train_acc=0.9262 | val_loss=0.682145, val_acc=0.8947\n",
      "Epoch 007 | train_loss=0.522110, train_acc=0.9286 | val_loss=0.618420, val_acc=0.8967\n",
      "Epoch 008 | train_loss=0.451350, train_acc=0.9401 | val_loss=0.551897, val_acc=0.9011\n",
      "Epoch 009 | train_loss=0.388111, train_acc=0.9525 | val_loss=0.520931, val_acc=0.9065\n",
      "Epoch 010 | train_loss=0.359091, train_acc=0.9518 | val_loss=0.489103, val_acc=0.9061\n",
      "Epoch 011 | train_loss=0.326899, train_acc=0.9557 | val_loss=0.454155, val_acc=0.9129\n",
      "Epoch 012 | train_loss=0.291673, train_acc=0.9654 | val_loss=0.430612, val_acc=0.9159\n",
      "Epoch 013 | train_loss=0.268404, train_acc=0.9618 | val_loss=0.414350, val_acc=0.9208\n",
      "Epoch 014 | train_loss=0.243566, train_acc=0.9693 | val_loss=0.395157, val_acc=0.9198\n",
      "Epoch 015 | train_loss=0.227849, train_acc=0.9717 | val_loss=0.383354, val_acc=0.9208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m yolov5_classifier_model = \u001b[43mtrain_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43myolo_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaders_sets_per_seed\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaders_sets_per_seed\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_with_early_stopping\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, loss_function, optimizer_factory, device, max_epochs, improvement_delta, patience_epochs)\u001b[39m\n\u001b[32m     17\u001b[39m early_stopper = EarlyStopper(patience_epochs=patience_epochs, min_delta=improvement_delta)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     train_loss, train_accuracy = \u001b[43mrun_one_epoch_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     val_loss, val_accuracy = run_one_epoch_validation(\n\u001b[32m     27\u001b[39m         model=model,\n\u001b[32m     28\u001b[39m         val_dataloader=val_dataloader,\n\u001b[32m     29\u001b[39m         loss_function=loss_function,\n\u001b[32m     30\u001b[39m         device=device,\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m     epoch_loss.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mrun_one_epoch_train\u001b[39m\u001b[34m(model, train_dataloader, loss_function, optimizer, device)\u001b[39m\n\u001b[32m     11\u001b[39m total_correct = \u001b[32m0\u001b[39m\n\u001b[32m     12\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mFlowersDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     image_pil, label = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m     image_pil = \u001b[38;5;28mself\u001b[39m.transform(image_pil)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_pil, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[39m, in \u001b[36mSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset[[\u001b[38;5;28mself\u001b[39m.indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[39m, in \u001b[36mConcatDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     sample_idx = idx - \u001b[38;5;28mself\u001b[39m.cumulative_sizes[dataset_idx - \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\flowers102.py:81\u001b[39m, in \u001b[36mFlowers102.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) -> Tuple[Any, Any]:\n\u001b[32m     80\u001b[39m     image_file, label = \u001b[38;5;28mself\u001b[39m._image_files[idx], \u001b[38;5;28mself\u001b[39m._labels[idx]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     image = \u001b[43mPIL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     84\u001b[39m         image = \u001b[38;5;28mself\u001b[39m.transform(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:979\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    977\u001b[39m         mode = \u001b[33m\"\u001b[39m\u001b[33mRGBA\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m (mode == \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matrix):\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# matrix conversion\u001b[39;00m\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aviv Metz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:1263\u001b[39m, in \u001b[36mImage.copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1255\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1256\u001b[39m \u001b[33;03mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[32m   1257\u001b[39m \u001b[33;03minto an image, but still retain the original.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28mself\u001b[39m.load()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "yolov5_classifier_model = train_with_early_stopping(\n",
    "    model=yolo_model,\n",
    "    train_dataloader=loaders_sets_per_seed[0][0],\n",
    "    val_dataloader=loaders_sets_per_seed[0][1],\n",
    "    loss_function=loss_function,\n",
    "    optimizer_factory=optimizer_factory,\n",
    "    device=device,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation Functions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
